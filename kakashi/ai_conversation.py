from typing import Dict, List

from langchain.chains import ConversationalRetrievalChain, RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import CohereEmbeddings, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from rich.console import Console
from utils import (DocsJSONLLoader, get_cohere_api_key, get_file_path,
                   get_openai_api_key, get_query_from_user, load_config)

console = Console()


def load_documents(file_path: str) -> List[Dict]:
    """
    Loads documents from a JSONL file and divides them into chunks.

    Args:
        file_path (str): Path to the JSONL file.

    Returns:
        The loaded documents divided into chunks.
    """
    config = load_config()
    loader = DocsJSONLLoader(file_path)
    data = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config["text_splitting"]["chunk_size"],
        chunk_overlap=config["text_splitting"]["chunk_overlap"],
        length_function=len,
    )

    return text_splitter.split_documents(data)


# TODO: Implement the use of Cohere embeddings.
def select_embedding_provider(provider: str, model: str):
    """
    Selects the embeddings provider for the chatbot.

    Args:
        provider (str): The embeddings provider. 'openai' or 'cohere'.
        model (str): The model to use for the embeddings.

    Returns:
        The embeddings object from the selected provider.
    """
    if provider.lower() == "openai":
        get_openai_api_key()
        return OpenAIEmbeddings(model=model)
    elif provider.lower() == "cohere":
        get_cohere_api_key()
        return CohereEmbeddings(model=model)
    else:
        raise ValueError(
            f"Incompatible embedding provider: {provider}. Supported providers are 'OpenAI' and 'Cohere'."
        )


def get_chroma_db(embeddings, documents, path):
    """
    Gets the Chroma database. Creates a new one or loads an existing one.

    Args:
        embeddings: The embeddings object to use.
        documents: The documents to index in the database.
        path: The path to the Chroma database.

    Returns:
        The Chroma database object.
    """
    config = load_config()
    if config["recreate_chroma_db"]:
        console.print("Recreating Chroma DB...")
        return Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=path,
        )
    else:
        console.print("Loading existing Chroma DB...")
        return Chroma(persist_directory=path, embedding_function=embeddings)


def process_qa_query(query: str, retriever, llm) -> str:
    """
    Processes a user query and generates a chatbot response.

    Args:
        query (str): The user's query.
        retriever: The object in charge of retrieving the documents.
        llm: The large language model to use.

    Returns:
        The response generated by the chatbot.
    """
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=retriever
    )
    console.print("[yellow]The AI â€‹â€‹is thinking...[/yellow]")
    return qa_chain.run(query)


def process_conversation_query(
    query: str, retriever, llm: ChatOpenAI, chat_history: List
) -> str:
    """
    Processes a user query and generates a chatbot response in conversation mode.

    Args:
        query (str): The user's query.
        retriever: The data retrieval object to search for the answer.
        llm: The large language model to use.
        chat_history: The list of previous question-answer pairs in the conversation.

    Returns:
        The response generated by the chatbot.
    """
    config = load_config()

    conversation = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        verbose=config["conversation_chain"]["verbose"],
    )

    console.print("[yellow]The AI â€‹â€‹is thinking...[/yellow]")
    # print(f"The history before this query: {chat_history}")
    result = conversation({"question": query, "chat_history": chat_history})
    chat_history.append((query, result["answer"]))

    return result["answer"]


def run_conversation(vectorstore, chat_type, llm):
    """
    Starts a conversation with the user.

    Args:
        vectorstore: The vector database to look for answers.
        chat_type: The type of chat to use.
    """
    config = load_config()

    console.print(
        "\n[blue]AI:[/blue] Hey Starker ðŸ‘‰ðŸ‘ˆ! Ask me anything about the ecosystem: tooling, Cairo, Starknet, etc."
    )

    chat_history = []

    if chat_type == "memory_chat":
        console.print(
            "\n[green]You are using the chatbot in conversation mode. Remember that this chatbot can remember parts of the conversation to generate more contextualized responses.[/green]"
        )
    elif chat_type == "qa_chat":
        console.print(
            "\n[green]You are using the chatbot in question and answer mode. This chatbot generates responses based purely on the current query without considering the conversation history.[/green]"
        )

    retriever = vectorstore.as_retriever(
        search_kwargs={"k": config["document_retrieval"]["k"]}
    )

    while True:
        console.print("\n[blue]You:[/blue]")
        query = get_query_from_user()

        if query.lower() == "exit":
            break

        if chat_type == "memory_chat":
            response = process_conversation_query(
                query=query, retriever=retriever, llm=llm, chat_history=chat_history
            )
        elif chat_type == "qa_chat":
            response = process_qa_query(query=query, retriever=retriever, llm=llm)

        console.print(f"[red]AI:[/red] {response}")


def main():
    """
    Main function that runs whenthe script starts.
    """
    config = load_config()

    embeddings = select_embedding_provider(
        config["embeddings_provider"], config["embeddings_model"]
    )

    documents = load_documents(get_file_path())

    vectorstore_chroma = get_chroma_db(embeddings, documents, config["chroma_db_name"])

    console.print(f"[green]{len(documents)} documents loaded.[/green]")

    llm = ChatOpenAI(
        model_name=config["chat_model"]["model_name"],
        temperature=config["chat_model"]["temperature"],
        max_tokens=config["chat_model"]["max_tokens"],
    )

    run_conversation(vectorstore_chroma, config["chat_type"], llm)


if __name__ == "__main__":
    main()
